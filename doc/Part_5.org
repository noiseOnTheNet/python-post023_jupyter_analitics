#+BLOG: noise on the net
#+POSTID: 572
#+ORG2BLOG:
#+DATE: [2024-12-29 dom 21:57]
#+OPTIONS: toc:nil num:nil todo:nil pri:nil tags:nil ^:nil
#+CATEGORY: Language learning
#+TAGS: Python
#+DESCRIPTION:
#+TITLE: Correlations and Linear Models

<<64909858-2157-4525-b5c0-dc267988e355>>
* Correlations and linear models
:PROPERTIES:
:CUSTOM_ID: correlations-and-linear-models
:END:
A linear model estimate a response from the linear combination of one or
more inputs

\(y \approx x_1 \alpha_1 + y_2 \alpha_2 + ... + y_n \alpha_n\)

\(y \approx \vec{x} \cdot \vec{\alpha}\)

<<552a4ab7-c3f3-4317-86c5-7268e53fae43>>
given an estimation \(\hat{y} = \vec{x} \cdot \vec{\alpha}\)

we look for the best \(\vec{\alpha}\) which minimize all residuals
\(\epsilon(\hat{y}) = y - \hat{y}\)

#+begin_src jupyter-python
import pandas as pd
#+end_src

<<7855757b-2569-43ba-af23-d245aca8e0c7>>
This dataset collects the yearly water and energy consuption estimation
per capita in Milan collected by the italian government

Data is grouped by

- water consumption
- methan consumption
- electricity consumption

#+begin_src jupyter-python
cons = pd.read_csv("ds523_consumoacquaenergia.csv",sep=";")
#+end_src

#+begin_src jupyter-python
cons.describe(include="all")
#+end_src

#+RESULTS:
:               anno              Consumo pro capite tipo  Consumo pro capite
: count     36.00000                                   36           36.000000
: unique         NaN                                    3                 NaN
: top            NaN  Energia elettrica per uso domestico                 NaN
: freq           NaN                                   12                 NaN
: mean    2005.50000                                  NaN          573.072222
: std        3.50102                                  NaN          471.777743
: min     2000.00000                                  NaN           80.400000
: 25%     2002.75000                                  NaN           89.625000
: 50%     2005.50000                                  NaN          432.900000
: 75%     2008.25000                                  NaN         1195.650000
: max     2011.00000                                  NaN         1228.600000
#+begin_src jupyter-python
cons["Consumo pro capite tipo"].unique()
#+end_src

#+RESULTS:
: array(['Energia elettrica per uso domestico',
:        'Gas metano per uso domestico e riscaldamento',
:        'Acqua fatturata per uso domestico'], dtype=object)
#+begin_src jupyter-python
translate = {
    'Energia elettrica per uso domestico':'electricity',
    'Gas metano per uso domestico e riscaldamento':'methan',
    'Acqua fatturata per uso domestico':'water'
}
cons["type"] = cons["Consumo pro capite tipo"].map(translate)
#+end_src

#+begin_src jupyter-python
cons2 = cons.pivot(index="anno",columns="type",values="Consumo pro capite").reset_index()
cons2 = cons2.rename({"anno":"year"}, axis="columns")
cons2
#+end_src

#+RESULTS:
: type  year  electricity  methan  water
: 0     2000       1130.2   509.0   92.1
: 1     2001       1143.9   500.7   91.3
: 2     2002       1195.5   504.2   90.4
: 3     2003       1222.8   480.2   87.3
: 4     2004       1228.6   442.4   80.4
: 5     2005       1225.0   434.5   81.3
: 6     2006       1219.7   431.3   82.2
: 7     2007       1197.0   381.1   81.6
: 8     2008       1203.0   384.9   84.5
: 9     2009       1202.9   389.6   85.8
: 10    2010       1200.7   406.2   83.2
: 11    2011       1196.1   377.9   83.1
#+begin_src jupyter-python
import seaborn as sns
sns.pairplot(cons2)
#+end_src

#+RESULTS:
: <seaborn.axisgrid.PairGrid at 0x220423dea50>

[[file:images/a30a601a27790b4a69e3dda7196cb228ce860a20.png]]

#+begin_src jupyter-python
sns.regplot(cons2,x="year",y="methan")
#+end_src

#+RESULTS:
: <Axes: xlabel='year', ylabel='methan'>

[[file:images/d1ecfa3b30467fa96fda8febbffe60b0c3718c5a.png]]

<<679fb861-865c-466e-b0f7-2806a5ffcafb>>
** Covariance and correlation
:PROPERTIES:
:CUSTOM_ID: covariance-and-correlation
:END:
The [[https://en.wikipedia.org/wiki/Covariance_matrix][covariance
matrix]], defined as

\(cov[X_i, X_j] = E[(X_i - E[X_i])(X_j - E[X_j])]\)

is the multidimensional extension of the variance, elements of the
diagonal being the variance of the corrisponding dimension; its
eigenvectors define an ellipsoid representing the most important
combinations of the dimensional features; this is used in Principal
Compaonent Analysis, a technique which helps to define the most
impactful features.

By dividing each element with the product of the standard deviations we
have the [[https://en.wikipedia.org/wiki/Correlation][correlation
matrix]]

\(corr[X_i, X_j] = \frac{E[(X_i - E[X_i])(X_j - E[X_j])]}{\sigma_i\sigma_j}\)

The elements outside the diagonal are numbers between -1 and 1; 0
represents no correlation (like a spherical cloud) while 1 and -1
represent positive and negative correlation respectively; this gives us
a first estimation of the possible linear dependecies within a set of
observation features

#+begin_src jupyter-python
import numpy as np
# numpy expects a matrix where each feature is in a row instead of a column
# thus we need to transpose it
np.corrcoef(np.transpose(np.array(cons2)))
#+end_src

#+RESULTS:
: array([[ 1.        ,  0.44786015, -0.93548315, -0.65540971],
:        [ 0.44786015,  1.        , -0.46029677, -0.77514369],
:        [-0.93548315, -0.46029677,  1.        ,  0.75208366],
:        [-0.65540971, -0.77514369,  0.75208366,  1.        ]])

<<1905b506-3a4d-4a8a-acb8-5fbac1e9eb86>>
we can see that the negative correlation between year and methan is
about -0.9 which makes it a good candidate for a linear correlation

#+begin_src jupyter-python
from scipy import stats
#+end_src

<<fdaa4ec5-929a-4096-9622-716dbd88297b>>
** Regression calculation
:PROPERTIES:
:CUSTOM_ID: regression-calculation
:END:
in this simple case we have

- few observations
- only one input value so we may directly use the
  [[https://en.wikipedia.org/wiki/Ordinary_least_squares][Ordinary Least
  Squares regression method]] to evaluate the best fit

#+begin_src jupyter-python
result = stats.linregress(x=cons2.year, y=cons2.methan)
result
#+end_src

#+RESULTS:
: LinregressResult(slope=np.float64(-13.141258741258738), intercept=np.float64(26791.62773892773), rvalue=np.float64(-0.9354831530794605), pvalue=np.float64(7.894692952340761e-06), stderr=np.float64(1.5697563928623894), intercept_stderr=np.float64(3148.151109622701))

<<b0e77a9c-05d4-499c-9b3f-af3d4ecc6039>>
the returned object contains some interesting values; let's check the
first two:

- slope
- intercept

allows us to write a simple prediction formula

#+begin_src jupyter-python
def predict_methan(year):
    return result.slope * year + result.intercept
#+end_src

<<455cf468-485c-4d2b-a9f1-1b6d9619c0d7>>
with this formula we can build a chart of our linear regression

#+begin_src jupyter-python
import matplotlib.pyplot as plt
import seaborn as sns
#+end_src

#+begin_src jupyter-python
# create a plot canvas
fig, ax = plt.subplots(1,1)

#first plot the points into our canvas
sns.scatterplot(x=cons2.year, y=cons2.methan, ax=ax)

# then plot a line from the first to the last point on the same canvas
year0 = min(cons2.year)
year1 = max(cons2.year)
ax.plot((year0,year1),(predict_methan(year0),predict_methan(year1)))
#+end_src

#+RESULTS:
: [<matplotlib.lines.Line2D at 0x2204c732350>]

[[file:images/97bfa3f43f44e8056f8bf8face7fc9f0482200dd.png]]

<<900888fc-d76d-4255-80ab-60b36ea1230a>>
note: the polymorphism allows to properly use the prodict_methan
function also with pandas Series

<<ad02f23d-905d-4f37-ad4a-3942e8a59b52>>
** Assessing the qaulity of a regression
:PROPERTIES:
:CUSTOM_ID: assessing-the-qaulity-of-a-regression
:END:

#+begin_src jupyter-python
residuals = cons2.methan - predict_methan(cons2.year)
#+end_src

<<5a1034f2-2c36-45bc-ae17-a380eb3f1be0>>
looking at residuals distribution may show some pattern; in this case we
may assume there is a better way to represent the relation between the
features under investigation.

In our example looks like there is no apparent pattern

#+begin_src jupyter-python
sns.scatterplot(x=cons2.year, y=residuals)
#+end_src

#+RESULTS:
: <Axes: xlabel='year', ylabel='None'>

[[file:images/6d66ee4c83a50c1f8dc3abc03acc9ef01342525b.png]]

<<5c2ead8b-5836-430d-b740-6debee27bacd>>
The next step would be to assess the variance of residuals respect to
the total variance of the distribution of the output variable Y:

\begin{equation} \frac{var[\epsilon]}{var[Y]} = \frac{E[(\epsilon -
E[\epsilon])^2]}{E[(Y - E[Y])^2]} \end{equation}

let's use \(\hat{Y}\) to represent the predicted values; by knowing that
the mean of the residuals is 0 and their definition

\begin{equation} E[\epsilon] = 0 \end{equation}

\begin{equation} \epsilon = Y - \hat{Y} \end{equation}

we have

\begin{equation} \frac{var[\epsilon]}{var[Y]} = \frac{E[(Y -
\hat{Y})^2]}{E[(Y - E[Y])^2]} \end{equation}

now the quantity

\begin{equation} R^2 = 1 - \frac{E[(Y - \hat{Y})^2]}{E[(Y - E[Y])^2]}
\end{equation}

represent the fraction of the variance of the original dataset explained
by the linear relation: this is a real number between 0 and 1 where 0
represents no actual explaination (i.e. the mean has the same prediction
power) to 1 representing all the relation is explained

# images/a30a601a27790b4a69e3dda7196cb228ce860a20.png https://noiseonthenet.space/noise/wp-content/uploads/2025/01/a30a601a27790b4a69e3dda7196cb228ce860a20.png
# images/d1ecfa3b30467fa96fda8febbffe60b0c3718c5a.png https://noiseonthenet.space/noise/wp-content/uploads/2025/01/d1ecfa3b30467fa96fda8febbffe60b0c3718c5a.png
# images/97bfa3f43f44e8056f8bf8face7fc9f0482200dd.png https://noiseonthenet.space/noise/wp-content/uploads/2025/01/97bfa3f43f44e8056f8bf8face7fc9f0482200dd.png
# images/6d66ee4c83a50c1f8dc3abc03acc9ef01342525b.png https://noiseonthenet.space/noise/wp-content/uploads/2025/01/6d66ee4c83a50c1f8dc3abc03acc9ef01342525b.png
